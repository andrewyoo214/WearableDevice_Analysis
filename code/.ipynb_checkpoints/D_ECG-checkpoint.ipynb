{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2603a7c5-12ee-4aec-be15-234b8f7f558a",
   "metadata": {},
   "source": [
    "# Diagnosis based on ECG data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9bd26-b72e-474e-a942-14d6b1ed5123",
   "metadata": {},
   "source": [
    "- - - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd0dec-dfe7-42b9-ad33-b4a46524aaef",
   "metadata": {},
   "source": [
    "* Diagnosis using collected ECG data\n",
    "* Currently total 42 subjects\n",
    "* 3 classes (DEP, SUI, NOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca25eea5-7d1f-401a-8134-ec7fbff1f678",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a812ed44-f853-4ce4-8ce2-27146f72459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required components \n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import urllib\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc3c86b-4a67-4726-9061-ab9fbc3d6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f4ff95-374a-40cb-a65a-cf8cc591bb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9b688-f5bd-48dc-a605-f75e418ef0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40df9de8-0768-4084-b76d-e2a81676c9e1",
   "metadata": {},
   "source": [
    "## Basic data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25788be7-1aa1-4f8f-9d72-db7caf375b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_df = pd.read_csv('E:/RESEARCH/Datasets/wearable/AI_coded_1.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c563b2-1a0e-4890-8d18-b4afa99c19d6",
   "metadata": {},
   "source": [
    "* 1: depression, 2: suicidality, 3: normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dce0f46-956b-4c1e-9a4f-a9b7f0468f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_df['class'] = ecg_df['class'].astype(\"category\")\n",
    "ecg_df['sub'] = ecg_df['sub'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3417ed-fc23-4f4c-8218-2bb952701d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "depression = ecg_df[ecg_df['class']==1]\n",
    "print((depression.index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66cdb7-bc36-424c-a4fe-97e94228dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "suicidal = ecg_df[ecg_df['class']==2]\n",
    "print((suicidal.index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f283e0-3bf8-438e-b918-fd7e6466de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = ecg_df[ecg_df['class']==3]\n",
    "print((normal.index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09cd5e2-caf8-456f-9f20-c260fb0955bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c770d8-588e-46ad-99e5-1347cb72334d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b7fa493-ab29-4e90-8743-2fccebc6fa60",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153cdc7-38a1-4dcf-b06f-c3b2f872a620",
   "metadata": {},
   "source": [
    "## Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a532aec-63f2-4f00-955d-a752c95171e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking file path and names\n",
    "file_path = \"E:/RESEARCH/Datasets/wearable/ECG/test_0420/train/dep/\"\n",
    "file_names = os.listdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7300a5-dfa7-4809-bace-3cda6a9a6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing file names to 1, 2, ...\n",
    "i = 1\n",
    "for name in file_names:\n",
    "    src = os.path.join(file_path, name)\n",
    "    dst = str(i) + '.png'\n",
    "    dst = os.path.join(file_path, dst)\n",
    "    os.rename(src, dst)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2789ee5d-6ef2-4000-b80c-ab383bc1c5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Converting pdf file into png file\n",
    "for name in file_names:\n",
    "    pages = convert_from_path(file_path + name, poppler_path=\"E:/RESEARCH/Datasets/wearable/ECG/poppler/Library/bin\")\n",
    "    \n",
    "    for page in pages:\n",
    "        page.save(file_path + name + '.png', \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ae611-4319-44ed-812d-b8623ebd4ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Data Crop (deleting patients' information)\n",
    "# original png size = 2200 x 1700\n",
    "# leaving the important part only (only the ecg data part)\n",
    "left = 100\n",
    "top = 450\n",
    "right = 2100\n",
    "bottom = 1300\n",
    "## 100, 450, 2100, 1300 스케일로 자르면 딱 ecg 30초 전체 부분 나옴.\n",
    "\n",
    "for name in file_names:\n",
    "    im = Image.open(file_path + name)\n",
    "    imc = im.crop((left, top, right, bottom))\n",
    "    imc.save(file_path+name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527baf7-6b7e-41fa-b06c-3ca2abccf29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## directory setting and get file names for 2 second cycle of ECG\n",
    "crop_path = \"E:/RESEARCH/Datasets/wearable/ECG/test_0420/train_crop/sui/\"\n",
    "crop_names = os.listdir(crop_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8336ff-97ad-4064-909b-5103d1e16fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the points for cropped image\n",
    "left = 1000\n",
    "top = 0\n",
    "right =1400\n",
    "bottom = 250\n",
    "## 위에서 한번 처리한 이미지에 대해 1000, 0, 1400, 250로 자르면 딱 5~7초 ecg cycle 나옴.\n",
    "\n",
    "\n",
    "for name in crop_names:\n",
    "    im = Image.open(crop_path + name)\n",
    "    imc = im.crop((left, top, right, bottom))\n",
    "    imc.save(crop_path+name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e907379e-8027-4dbd-a2cc-4588f0c02ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing file names to 1, 2, ...\n",
    "i = 1\n",
    "for name in crop_names:\n",
    "    src = os.path.join(crop_path, name)\n",
    "    dst = str(i) + '.png'\n",
    "    dst = os.path.join(crop_path, dst)\n",
    "    os.rename(src, dst)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e37414a-601e-4d7c-9ecc-8678e5e0f2a4",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356055b6-cff6-4b08-99d6-8049f1e83305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf86b83-a7bd-43ed-8da7-2e247cdbb17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53c89b-6157-4245-a27d-e793b2cefb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4956d57c-5e30-44cc-a5b9-39cfc97c86ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36957f4a-40b0-4192-9aa3-6457cf7ac32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70db10d-aca4-4343-a171-8a22f391226d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf83414-2198-47a4-b366-590a6117a63b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141238f-7cf6-4341-9b76-4a55bbcf8f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc927db1-35c3-4235-984c-94757dd92dd7",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5079a1-9dd5-43f6-8058-d7eda91cfbf8",
   "metadata": {},
   "source": [
    "## With simple image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8936ca-fda9-41e7-a24e-4422a03dc8bd",
   "metadata": {},
   "source": [
    "* Our dataset numbers (dep: 244, nor: 402, sui: 105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ce99b05-e36a-4827-ad61-df08aca02a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d2bbf92c50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Args:\n",
    "    # arugments\n",
    "    epochs=50\n",
    "    bs=16\n",
    "    lr=0.001\n",
    "    momentum=0.9\n",
    "    num_channels=3 \n",
    "    num_classes=3\n",
    "    verbose='store_true'\n",
    "    seed=710674\n",
    "\n",
    "args = Args()    \n",
    "\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70151a5a-f25e-4e82-b6d2-d35c01b5c89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.7.1  Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "#Setting torch environment\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device: ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50297e8d-25ee-4164-8a7e-6e3d772ddfcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705ad3f-d762-4fe1-8cba-8c5929893741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f131b38a-fc62-4ce8-a8c3-e61fcc8f315d",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fb6c8-9881-4711-ace9-34c710bc8eaa",
   "metadata": {},
   "source": [
    "### Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4417cf1-5a8f-4103-8191-983c78afcfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    }
   ],
   "source": [
    "model_eff3 = EfficientNet.from_pretrained('efficientnet-b3', num_classes=args.num_classes)\n",
    "model_resnet18 = models.resnet18(pretrained=True)\n",
    "model_mobnetv2 = models.mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abce0c5d-56df-4872-abdb-bfcac48ca6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## resnet 구조는 마지막 fc layer의 out_features 를 바꿔주면 되고.\n",
    "model_resnet18.fc = nn.Linear(in_features = 512, out_features = args.num_classes)\n",
    "model_mobnetv2.classifier = nn.Linear(in_features = 1280, out_features = args.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0c466-ca78-4b73-a00e-31479aa7aba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ae0d61c-e7a4-41fa-8a24-00187646c1d5",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aae3e76-9a73-4b2e-b78b-6d042fff15f4",
   "metadata": {},
   "source": [
    "### Simple CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d489787-2ffc-4b83-8997-eaf6c3b383a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Designing simple CNN model architecture.\n",
    "class CNN_ecg(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(CNN_ecg, self).__init__()\n",
    "\n",
    "        def conv_batch(input_size, output_size, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_size, output_size, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(output_size),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "\n",
    "        def conv_depth(input_size, output_size, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_size, input_size, 3, stride, 1, groups=input_size, bias=False),\n",
    "                nn.BatchNorm2d(input_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                \n",
    "                nn.Conv2d(input_size, output_size, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(output_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            conv_batch(3, 32, 2),\n",
    "            conv_depth(32, 64, 1),\n",
    "            conv_depth(64, 128, 2),\n",
    "            conv_depth(128, 128, 1),\n",
    "            conv_depth(128, 256, 2),\n",
    "            conv_depth(256, 256, 1),\n",
    "            conv_depth(256, 512, 2),\n",
    "            conv_depth(512, 512, 1),\n",
    "            conv_depth(512, 512, 1),\n",
    "            conv_depth(512, 1024, 2),\n",
    "            conv_depth(1024, 1024, 1),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "#         self.fc1 = nn.Linear(1024, 100)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(-1, 1024)\n",
    "#         x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c1a16-1f83-42e2-8d2b-fd63dc47379e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "922f8e24-6919-44b5-acdb-8b1fc2766c5a",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c6ac8-03c3-4243-88a7-1df32dee7f49",
   "metadata": {},
   "source": [
    "### Training Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ed26f5c-43a4-4bb3-9357-2f8b5699f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_eff3.to(DEVICE)\n",
    "# model = model_mobnetv2.to(DEVICE)\n",
    "# model = model_resnet18.to(DEVICE)\n",
    "# model = CNN_ecg(args.num_channels, num_classes = args.num_classes).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8cbda-f300-48d8-a53d-b3fbe40563fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f463fa85-d0a8-4cee-ada3-e031e9a81ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "004459b4-8447-4212-bb1d-c2a5b2b5c01d",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70452da4-0be7-409d-8bdb-66157267c5b2",
   "metadata": {},
   "source": [
    "### Input Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4fe02e6d-7faa-4416-8f70-78d6cfb71456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(256),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(contrast=(0.3, 1), saturation=(0.3, 1)),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456,0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5f590-3361-4b96-9a7a-f0899f25131a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897fb805-20c1-49ed-a895-b47f4c2e25e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e75f98b-fe76-43c3-8bce-cb81e9456ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading image data\n",
    "ecg_data = datasets.ImageFolder(root = 'E:/RESEARCH/Datasets/wearable/ECG/test_0420/train', transform = data_transforms)\n",
    "# ecg_data = datasets.ImageFolder(root = 'E:/RESEARCH/Datasets/wearable/ECG/test_0420/train_crop', transform = data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "30b9f889-526a-4b06-95e5-365972a4ccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "151\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(ecg_data))\n",
    "test_size = len(ecg_data)-train_size\n",
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b753f461-3468-4dbb-ad3b-6af7edc72ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(ecg_data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3436a011-6965-4f17-a6fe-e204b9906e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.bs, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.bs, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0ef1ca4-e3c8-487e-b02d-844947b9845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 1, 1, 1, 0, 0, 1, 2, 0, 1, 1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a7aacc-663c-45a6-b5a1-e07e9bfe88c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353cb358-d690-4705-b373-40ba7e952110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6e28012-a247-47b0-be82-8dc19766a521",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3739b8-a40d-44c3-8f1f-3ae58eb1a31f",
   "metadata": {},
   "source": [
    "### Optimizer and Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce6c87af-3007-4797-8e73-9adf7d132ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Optimizer and Objective Function\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, total_steps=50, anneal_strategy='cos')\n",
    "criterion = nn.CrossEntropyLoss() ## setup the loss function\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510f129-5344-414b-aa7e-2fa942690238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6700902-ea09-4d61-8e2b-7da68747d37f",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a9f6f-d37f-455c-8dc3-579afcd654a9",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "537645c2-462f-4570-b2ba-997e0034e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during CNN model\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))\n",
    "\n",
    "    scheduler.step() #for learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "45ba0939-a6eb-437e-b2cf-fbab13373ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during the learning process\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    validation =[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "            \n",
    "    \n",
    "    test_loss /= (len(test_loader)) \n",
    "    validation_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    validation.append(validation_accuracy)\n",
    "    \n",
    "    return test_loss, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee0e44-78b0-41a4-838a-7b390275e29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1904fcda-c97a-4626-9b9b-74e297552492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9999999999999996e-05\n",
      "Train Epoch: 1 [0/600 (0%)]\tTrain Loss: 1.172327\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 1.1323, \tValidation Accuracy: 28.48 % \n",
      "\n",
      "5.2034602152724665e-05\n",
      "Train Epoch: 2 [0/600 (0%)]\tTrain Loss: 0.975320\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 1.0693, \tValidation Accuracy: 45.70 % \n",
      "\n",
      "8.753494340683887e-05\n",
      "Train Epoch: 3 [0/600 (0%)]\tTrain Loss: 0.855695\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 1.0433, \tValidation Accuracy: 43.71 % \n",
      "\n",
      "0.00014472088841534561\n",
      "Train Epoch: 4 [0/600 (0%)]\tTrain Loss: 0.864667\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.9801, \tValidation Accuracy: 52.32 % \n",
      "\n",
      "0.0002207248951078079\n",
      "Train Epoch: 5 [0/600 (0%)]\tTrain Loss: 0.568829\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.9867, \tValidation Accuracy: 41.72 % \n",
      "\n",
      "0.0003117358052235721\n",
      "Train Epoch: 6 [0/600 (0%)]\tTrain Loss: 0.367652\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.7783, \tValidation Accuracy: 64.90 % \n",
      "\n",
      "0.0004131899517009691\n",
      "Train Epoch: 7 [0/600 (0%)]\tTrain Loss: 0.859191\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.7110, \tValidation Accuracy: 72.85 % \n",
      "\n",
      "0.0005200000000000001\n",
      "Train Epoch: 8 [0/600 (0%)]\tTrain Loss: 0.387757\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 1.0038, \tValidation Accuracy: 56.95 % \n",
      "\n",
      "0.0006268100482990309\n",
      "Train Epoch: 9 [0/600 (0%)]\tTrain Loss: 0.399058\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.7325, \tValidation Accuracy: 66.23 % \n",
      "\n",
      "0.0007282641947764279\n",
      "Train Epoch: 10 [0/600 (0%)]\tTrain Loss: 0.520209\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.9145, \tValidation Accuracy: 68.87 % \n",
      "\n",
      "0.0008192751048921921\n",
      "Train Epoch: 11 [0/600 (0%)]\tTrain Loss: 1.126122\n",
      "\n",
      "[EPOCH: 11], \tTest Loss: 1.2649, \tValidation Accuracy: 63.58 % \n",
      "\n",
      "0.0008952791115846543\n",
      "Train Epoch: 12 [0/600 (0%)]\tTrain Loss: 0.386052\n",
      "\n",
      "[EPOCH: 12], \tTest Loss: 0.7282, \tValidation Accuracy: 72.19 % \n",
      "\n",
      "0.0009524650565931611\n",
      "Train Epoch: 13 [0/600 (0%)]\tTrain Loss: 0.179754\n",
      "\n",
      "[EPOCH: 13], \tTest Loss: 0.7654, \tValidation Accuracy: 69.54 % \n",
      "\n",
      "0.0009879653978472755\n",
      "Train Epoch: 14 [0/600 (0%)]\tTrain Loss: 0.421214\n",
      "\n",
      "[EPOCH: 14], \tTest Loss: 0.9405, \tValidation Accuracy: 62.25 % \n",
      "\n",
      "0.001\n",
      "Train Epoch: 15 [0/600 (0%)]\tTrain Loss: 0.327492\n",
      "\n",
      "[EPOCH: 15], \tTest Loss: 1.8482, \tValidation Accuracy: 70.20 % \n",
      "\n",
      "0.0009979871550490316\n",
      "Train Epoch: 16 [0/600 (0%)]\tTrain Loss: 0.469139\n",
      "\n",
      "[EPOCH: 16], \tTest Loss: 1.5027, \tValidation Accuracy: 57.62 % \n",
      "\n",
      "0.0009919648264401376\n",
      "Train Epoch: 17 [0/600 (0%)]\tTrain Loss: 0.100612\n",
      "\n",
      "[EPOCH: 17], \tTest Loss: 1.1325, \tValidation Accuracy: 37.09 % \n",
      "\n",
      "0.0009819815024222052\n",
      "Train Epoch: 18 [0/600 (0%)]\tTrain Loss: 0.585826\n",
      "\n",
      "[EPOCH: 18], \tTest Loss: 0.8300, \tValidation Accuracy: 57.62 % \n",
      "\n",
      "0.0009681175628501272\n",
      "Train Epoch: 19 [0/600 (0%)]\tTrain Loss: 0.099094\n",
      "\n",
      "[EPOCH: 19], \tTest Loss: 0.8130, \tValidation Accuracy: 69.54 % \n",
      "\n",
      "0.0009504846320134736\n",
      "Train Epoch: 20 [0/600 (0%)]\tTrain Loss: 0.197554\n",
      "\n",
      "[EPOCH: 20], \tTest Loss: 0.8747, \tValidation Accuracy: 58.94 % \n",
      "\n",
      "0.0009292246799033457\n",
      "Train Epoch: 21 [0/600 (0%)]\tTrain Loss: 0.119513\n",
      "\n",
      "[EPOCH: 21], \tTest Loss: 0.5782, \tValidation Accuracy: 72.19 % \n",
      "\n",
      "0.0009045088791534849\n",
      "Train Epoch: 22 [0/600 (0%)]\tTrain Loss: 0.354157\n",
      "\n",
      "[EPOCH: 22], \tTest Loss: 0.7781, \tValidation Accuracy: 76.16 % \n",
      "\n",
      "0.0008765362268588734\n",
      "Train Epoch: 23 [0/600 (0%)]\tTrain Loss: 0.168086\n",
      "\n",
      "[EPOCH: 23], \tTest Loss: 0.7691, \tValidation Accuracy: 77.48 % \n",
      "\n",
      "0.0008455319423681343\n",
      "Train Epoch: 24 [0/600 (0%)]\tTrain Loss: 0.238473\n",
      "\n",
      "[EPOCH: 24], \tTest Loss: 0.6466, \tValidation Accuracy: 76.16 % \n",
      "\n",
      "0.000811745653949763\n",
      "Train Epoch: 25 [0/600 (0%)]\tTrain Loss: 0.219671\n",
      "\n",
      "[EPOCH: 25], \tTest Loss: 0.7374, \tValidation Accuracy: 82.78 % \n",
      "\n",
      "0.0007754493889320882\n",
      "Train Epoch: 26 [0/600 (0%)]\tTrain Loss: 0.038597\n",
      "\n",
      "[EPOCH: 26], \tTest Loss: 0.4280, \tValidation Accuracy: 81.46 % \n",
      "\n",
      "0.0007369353834991743\n",
      "Train Epoch: 27 [0/600 (0%)]\tTrain Loss: 0.046004\n",
      "\n",
      "[EPOCH: 27], \tTest Loss: 0.4989, \tValidation Accuracy: 79.47 % \n",
      "\n",
      "0.0006965137297768985\n",
      "Train Epoch: 28 [0/600 (0%)]\tTrain Loss: 0.263993\n",
      "\n",
      "[EPOCH: 28], \tTest Loss: 0.4380, \tValidation Accuracy: 83.44 % \n",
      "\n",
      "0.0006545098791534849\n",
      "Train Epoch: 29 [0/600 (0%)]\tTrain Loss: 0.141187\n",
      "\n",
      "[EPOCH: 29], \tTest Loss: 0.1919, \tValidation Accuracy: 94.04 % \n",
      "\n",
      "0.0006112620219362892\n",
      "Train Epoch: 30 [0/600 (0%)]\tTrain Loss: 0.179260\n",
      "\n",
      "[EPOCH: 30], \tTest Loss: 0.3956, \tValidation Accuracy: 87.42 % \n",
      "\n",
      "0.000567118364442296\n",
      "Train Epoch: 31 [0/600 (0%)]\tTrain Loss: 0.159914\n",
      "\n",
      "[EPOCH: 31], \tTest Loss: 0.3048, \tValidation Accuracy: 89.40 % \n",
      "\n",
      "0.0005224343254455967\n",
      "Train Epoch: 32 [0/600 (0%)]\tTrain Loss: 0.241269\n",
      "\n",
      "[EPOCH: 32], \tTest Loss: 0.3395, \tValidation Accuracy: 89.40 % \n",
      "\n",
      "0.0004775696745544034\n",
      "Train Epoch: 33 [0/600 (0%)]\tTrain Loss: 0.026945\n",
      "\n",
      "[EPOCH: 33], \tTest Loss: 0.4544, \tValidation Accuracy: 80.79 % \n",
      "\n",
      "0.00043288563555770405\n",
      "Train Epoch: 34 [0/600 (0%)]\tTrain Loss: 0.181767\n",
      "\n",
      "[EPOCH: 34], \tTest Loss: 0.2694, \tValidation Accuracy: 90.07 % \n",
      "\n",
      "0.00038874197806371076\n",
      "Train Epoch: 35 [0/600 (0%)]\tTrain Loss: 0.078077\n",
      "\n",
      "[EPOCH: 35], \tTest Loss: 0.3480, \tValidation Accuracy: 87.42 % \n",
      "\n",
      "0.0003454941208465151\n",
      "Train Epoch: 36 [0/600 (0%)]\tTrain Loss: 0.195506\n",
      "\n",
      "[EPOCH: 36], \tTest Loss: 0.2965, \tValidation Accuracy: 92.05 % \n",
      "\n",
      "0.00030349027022310155\n",
      "Train Epoch: 37 [0/600 (0%)]\tTrain Loss: 0.168398\n",
      "\n",
      "[EPOCH: 37], \tTest Loss: 0.1337, \tValidation Accuracy: 94.04 % \n",
      "\n",
      "0.00026306861650082563\n",
      "Train Epoch: 38 [0/600 (0%)]\tTrain Loss: 0.024601\n",
      "\n",
      "[EPOCH: 38], \tTest Loss: 0.2311, \tValidation Accuracy: 91.39 % \n",
      "\n",
      "0.0002245546110679117\n",
      "Train Epoch: 39 [0/600 (0%)]\tTrain Loss: 0.217118\n",
      "\n",
      "[EPOCH: 39], \tTest Loss: 0.2144, \tValidation Accuracy: 93.38 % \n",
      "\n",
      "0.00018825834605023698\n",
      "Train Epoch: 40 [0/600 (0%)]\tTrain Loss: 0.067012\n",
      "\n",
      "[EPOCH: 40], \tTest Loss: 0.1625, \tValidation Accuracy: 94.04 % \n",
      "\n",
      "0.0001544720576318657\n",
      "Train Epoch: 41 [0/600 (0%)]\tTrain Loss: 0.018824\n",
      "\n",
      "[EPOCH: 41], \tTest Loss: 0.1841, \tValidation Accuracy: 92.05 % \n",
      "\n",
      "0.00012346777314112658\n",
      "Train Epoch: 42 [0/600 (0%)]\tTrain Loss: 0.046617\n",
      "\n",
      "[EPOCH: 42], \tTest Loss: 0.1419, \tValidation Accuracy: 94.04 % \n",
      "\n",
      "9.549512084651507e-05\n",
      "Train Epoch: 43 [0/600 (0%)]\tTrain Loss: 0.022806\n",
      "\n",
      "[EPOCH: 43], \tTest Loss: 0.0870, \tValidation Accuracy: 95.36 % \n",
      "\n",
      "7.077932009665415e-05\n",
      "Train Epoch: 44 [0/600 (0%)]\tTrain Loss: 0.005741\n",
      "\n",
      "[EPOCH: 44], \tTest Loss: 0.1919, \tValidation Accuracy: 92.72 % \n",
      "\n",
      "4.9519367986526286e-05\n",
      "Train Epoch: 45 [0/600 (0%)]\tTrain Loss: 0.064337\n",
      "\n",
      "[EPOCH: 45], \tTest Loss: 0.1722, \tValidation Accuracy: 94.04 % \n",
      "\n",
      "3.1886437149872676e-05\n",
      "Train Epoch: 46 [0/600 (0%)]\tTrain Loss: 0.065036\n",
      "\n",
      "[EPOCH: 46], \tTest Loss: 0.1384, \tValidation Accuracy: 94.70 % \n",
      "\n",
      "1.8022497577794767e-05\n",
      "Train Epoch: 47 [0/600 (0%)]\tTrain Loss: 0.053603\n",
      "\n",
      "[EPOCH: 47], \tTest Loss: 0.1418, \tValidation Accuracy: 93.38 % \n",
      "\n",
      "8.039173559862363e-06\n",
      "Train Epoch: 48 [0/600 (0%)]\tTrain Loss: 0.067773\n",
      "\n",
      "[EPOCH: 48], \tTest Loss: 0.1601, \tValidation Accuracy: 93.38 % \n",
      "\n",
      "2.016844950968456e-06\n",
      "Train Epoch: 49 [0/600 (0%)]\tTrain Loss: 0.084938\n",
      "\n",
      "[EPOCH: 49], \tTest Loss: 0.1851, \tValidation Accuracy: 92.05 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking train, val loss and accuracy\n",
    "\n",
    "total = []\n",
    "\n",
    "for epoch in range(1, args.epochs):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, validation_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tValidation Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, validation_accuracy))\n",
    "    \n",
    "    total.append((test_loss, validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa96dc-44bb-4da2-874f-c76d1894da6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee57a2a5-573c-404a-9b40-b50411b264fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51b0a9c0-8c05-445b-bd8c-69e9a310575e",
   "metadata": {},
   "source": [
    "### Model Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172c658-70e4-49dc-baf1-57aaf923705e",
   "metadata": {},
   "source": [
    "Performance Measure Explanations\n",
    "\n",
    "* Sensitivity = TP/(TP+FN) = (Number of true positive assessment) / (Number of all posi tive assessment)\n",
    "* Specificity = TN/(TN + FP) = (Number of true negative assessment)/(Number of all negative assessment)\n",
    "* Accuracy = (TN + TP)/(TN+TP+FN+FP) = (Number of correct assessments)/Number of all assessments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f12b4a56-a74b-4229-9a5f-392e7265fe2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48.  0.  0.]\n",
      " [ 3. 78.  0.]\n",
      " [ 0.  3. 19.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 24.0, 'Predicted label')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_classes = args.num_classes\n",
    "confusion_matrix = np.zeros((nb_classes, nb_classes))\n",
    "classes = {\n",
    "    \"0\": \"Depression\",\n",
    "    \"1\": \"Normal\",\n",
    "    \"2\": \"Suicidality\"\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (image, label) in enumerate(test_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        outputs = model(image)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for t, p in zip(label.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "print(confusion_matrix)\n",
    "\n",
    "class_names = list(classes.values())\n",
    "df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names).astype(int)\n",
    "heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=8)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=8)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "# plt.savefig('dep_train_entire_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "844c30c1-92df-4722-a55b-de66a440b190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall classification accuracy is : 0.9603\n",
      "sensitivity of depression class is : 1.0\n",
      "sensitivity of normal class is : 0.963\n",
      "sensitivity of suicidal class is : 0.8636\n",
      "specificity of depression class is : 0.97\n",
      "specificity of normal class is : 0.9571\n",
      "specificity of suicidal class is : 1.0\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix\n",
    "total = sum(sum(cm))\n",
    "\n",
    "## Accuracy, Sensitivity, and Specificity\n",
    "acc = (cm[0,0]+cm[1,1]+cm[2,2]) / total\n",
    "sen_dep = cm[0,0] / (cm[0,0] + cm[0,1] + cm[0,2])\n",
    "sen_nor = cm[1,1] / (cm[1,0] + cm[1,1] + cm[1,2])\n",
    "sen_sui = cm[2,2] / (cm[2,0] + cm[2,1] + cm[2,2])\n",
    "\n",
    "spe_dep = (cm[1,1] + cm[2,2]) / (cm[1,0] + cm[2,0] + cm[1,1] + cm[2,2])\n",
    "spe_nor = (cm[0,0] + cm[2,2]) / (cm[0,1] + cm[2,1] + cm[0,0] + cm[2,2])\n",
    "spe_sui = (cm[0,0] + cm[1,1]) / (cm[0,2] + cm[1,2] + cm[0,0] + cm[1,1])\n",
    "\n",
    "print(\"Overall classification accuracy is :\", round(acc, 4))\n",
    "print(\"sensitivity of depression class is :\", round(sen_dep, 4))\n",
    "print(\"sensitivity of normal class is :\", round(sen_nor,4))\n",
    "print(\"sensitivity of suicidal class is :\", round(sen_sui,4))\n",
    "\n",
    "print(\"specificity of depression class is :\", round(spe_dep,4))\n",
    "print(\"specificity of normal class is :\", round(spe_nor,4))\n",
    "print(\"specificity of suicidal class is :\", round(spe_sui,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfda23b-db79-46ae-a588-a9cb662616cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c254a3-41f5-448e-8dcf-49f379115096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3107cadd-99bc-4c85-aacb-139f651786a4",
   "metadata": {},
   "source": [
    "## Ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da136b-3d2a-45c8-bc88-406bdfd19385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c59a7c-a279-4f1a-ab12-47fb5404db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining ensemble\n",
    "ensemble = VotingClassifier(\n",
    "    estimator = model,\n",
    "    n_estimators = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1aa59-f6ab-4713-a31f-078c91665271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Set the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "ensemble.set_criterion(criterion)\n",
    "ensemble.set_optimizer(\n",
    "    \"Adam\", lr = args.lr\n",
    ")\n",
    "\n",
    "ensemble.set_scheduler(\n",
    "    \"CosineAnnealingLR\", T_max=args.epochs\n",
    ")\n",
    "\n",
    "# Train the ensemble\n",
    "ensemble.fit(\n",
    "    train_loader, epochs=args.epochs\n",
    ")\n",
    "\n",
    "# Evaluate the ensemble\n",
    "acc = ensemble.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c76bc-a50b-49de-b70e-50426d2ed909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "52b94425-fbc8-43aa-8054-1761b4c35d8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-812459c0f484>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mbatch_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mi\u001b[0m      \u001b[1;31m#batch size만큼 채워놓고\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mprediction_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mpredictions_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_list=[]\n",
    "    for i, (image, label) in enumerate(test_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        outputs = model(image)\n",
    "        \n",
    "        batch_idx = args.bs * i      #batch size만큼 채워놓고\n",
    "        prediction = (outputs > 0.5) #\n",
    "        prediction_array[batch_idx: batch_idx + image.shape[0], :] = prediction.astype(int)\n",
    "        predictions_list.append(prediction_array[..., np.newaxis])\n",
    "        \n",
    "        \n",
    "#         _, preds = torch.max(outputs, 1)\n",
    "#         for t, p in zip(label.view(-1), preds.view(-1)):\n",
    "#                 confusion_matrix[t.long(), p.long()] += 1\n",
    "                \n",
    "#  axis = 2를 기준으로 평균\n",
    "predictions_array = np.concatenate(predictions_list, axis = 2)\n",
    "predictions_mean = predictions_array.mean(axis = 2)\n",
    "\n",
    "# 평균 값이 0.5보다 클 경우 1 작으면 0\n",
    "predictions_mean = (predictions_mean > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff891a-2efe-427e-b7bc-c5823bf74714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053666d7-2496-4dd2-aa6e-6b9152826a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc83e5a-5034-4aa0-b9eb-1587be9f8350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c5c39-5ce8-4b17-a1d3-e0dcd4f2b8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d5ad07-4f53-445e-89d1-511410ddbfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2844fe8-1f20-41e4-9021-983d25038c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e692c-f3e8-4053-b9bf-2635bd855fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4cfae-1ccd-40cf-a16c-6f247113c76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0921df3d-af02-445d-bd45-98f56ee598c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0862ba4f-8787-4241-b4a5-bb9f7fdd72d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca54703c-82f9-4925-b887-b4b693ab38d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3dc8b-61ce-425c-b0ee-5b8695f1491d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6be033-c1d6-4602-b88e-84accab1268c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb5ea26-b16d-4bba-877f-2f7a1023def8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e488d3-2084-4990-94f3-9175bb3b9804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d687c0ba-b09e-414c-bc65-a79e7c38b187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92456f7-2340-4737-bf9e-cd12668cf593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
