{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2603a7c5-12ee-4aec-be15-234b8f7f558a",
   "metadata": {},
   "source": [
    "# Diagnosis based on ECG data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9bd26-b72e-474e-a942-14d6b1ed5123",
   "metadata": {},
   "source": [
    "- - - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd0dec-dfe7-42b9-ad33-b4a46524aaef",
   "metadata": {},
   "source": [
    "* Diagnosis using collected ECG data\n",
    "* Currently total 42 subjects\n",
    "* 3 classes (DEP, SUI, NOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca25eea5-7d1f-401a-8134-ec7fbff1f678",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a812ed44-f853-4ce4-8ce2-27146f72459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required components \n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import urllib\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3c86b-4a67-4726-9061-ab9fbc3d6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f4ff95-374a-40cb-a65a-cf8cc591bb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9b688-f5bd-48dc-a605-f75e418ef0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40df9de8-0768-4084-b76d-e2a81676c9e1",
   "metadata": {},
   "source": [
    "## Basic data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25788be7-1aa1-4f8f-9d72-db7caf375b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_df = pd.read_csv('E:/RESEARCH/Datasets/wearable/AI_coded_1.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c563b2-1a0e-4890-8d18-b4afa99c19d6",
   "metadata": {},
   "source": [
    "* 1: depression, 2: suicidality, 3: normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dce0f46-956b-4c1e-9a4f-a9b7f0468f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_df['class'] = ecg_df['class'].astype(\"category\")\n",
    "ecg_df['sub'] = ecg_df['sub'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3417ed-fc23-4f4c-8218-2bb952701d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "depression = ecg_df[ecg_df['class']==1]\n",
    "print((depression.index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66cdb7-bc36-424c-a4fe-97e94228dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "suicidal = ecg_df[ecg_df['class']==2]\n",
    "print((suicidal.index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f283e0-3bf8-438e-b918-fd7e6466de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = ecg_df[ecg_df['class']==3]\n",
    "print((normal.index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09cd5e2-caf8-456f-9f20-c260fb0955bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c770d8-588e-46ad-99e5-1347cb72334d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b7fa493-ab29-4e90-8743-2fccebc6fa60",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153cdc7-38a1-4dcf-b06f-c3b2f872a620",
   "metadata": {},
   "source": [
    "## Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a532aec-63f2-4f00-955d-a752c95171e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking file path and names\n",
    "file_path = \"E:/RESEARCH/Datasets/wearable/ECG/test_0420/train/dep/\"\n",
    "file_names = os.listdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7300a5-dfa7-4809-bace-3cda6a9a6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing file names to 1, 2, ...\n",
    "i = 1\n",
    "for name in file_names:\n",
    "    src = os.path.join(file_path, name)\n",
    "    dst = str(i) + '.png'\n",
    "    dst = os.path.join(file_path, dst)\n",
    "    os.rename(src, dst)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2789ee5d-6ef2-4000-b80c-ab383bc1c5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Converting pdf file into png file\n",
    "for name in file_names:\n",
    "    pages = convert_from_path(file_path + name, poppler_path=\"E:/RESEARCH/Datasets/wearable/ECG/poppler/Library/bin\")\n",
    "    \n",
    "    for page in pages:\n",
    "        page.save(file_path + name + '.png', \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ae611-4319-44ed-812d-b8623ebd4ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Data Crop (deleting patients' information)\n",
    "# original png size = 2200 x 1700\n",
    "# leaving the important part only (only the ecg data part)\n",
    "left = 100\n",
    "top = 450\n",
    "right = 2100\n",
    "bottom = 1300\n",
    "## 100, 450, 2100, 1300 스케일로 자르면 딱 ecg 30초 전체 부분 나옴.\n",
    "\n",
    "for name in file_names:\n",
    "    im = Image.open(file_path + name)\n",
    "    imc = im.crop((left, top, right, bottom))\n",
    "    imc.save(file_path+name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527baf7-6b7e-41fa-b06c-3ca2abccf29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## directory setting and get file names for 2 second cycle of ECG\n",
    "crop_path = \"E:/RESEARCH/Datasets/wearable/ECG/test_0420/train_crop/sui/\"\n",
    "crop_names = os.listdir(crop_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8336ff-97ad-4064-909b-5103d1e16fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the points for cropped image\n",
    "left = 1000\n",
    "top = 0\n",
    "right =1400\n",
    "bottom = 250\n",
    "## 위에서 한번 처리한 이미지에 대해 1000, 0, 1400, 250로 자르면 딱 5~7초 ecg cycle 나옴.\n",
    "\n",
    "\n",
    "for name in crop_names:\n",
    "    im = Image.open(crop_path + name)\n",
    "    imc = im.crop((left, top, right, bottom))\n",
    "    imc.save(crop_path+name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e907379e-8027-4dbd-a2cc-4588f0c02ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing file names to 1, 2, ...\n",
    "i = 1\n",
    "for name in crop_names:\n",
    "    src = os.path.join(crop_path, name)\n",
    "    dst = str(i) + '.png'\n",
    "    dst = os.path.join(crop_path, dst)\n",
    "    os.rename(src, dst)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e37414a-601e-4d7c-9ecc-8678e5e0f2a4",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356055b6-cff6-4b08-99d6-8049f1e83305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf86b83-a7bd-43ed-8da7-2e247cdbb17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53c89b-6157-4245-a27d-e793b2cefb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4956d57c-5e30-44cc-a5b9-39cfc97c86ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36957f4a-40b0-4192-9aa3-6457cf7ac32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70db10d-aca4-4343-a171-8a22f391226d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf83414-2198-47a4-b366-590a6117a63b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141238f-7cf6-4341-9b76-4a55bbcf8f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc927db1-35c3-4235-984c-94757dd92dd7",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5079a1-9dd5-43f6-8058-d7eda91cfbf8",
   "metadata": {},
   "source": [
    "## With simple image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8936ca-fda9-41e7-a24e-4422a03dc8bd",
   "metadata": {},
   "source": [
    "* Our dataset numbers (dep: 244, nor: 402, sui: 105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce99b05-e36a-4827-ad61-df08aca02a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # arugments\n",
    "    epochs=50\n",
    "    bs=16\n",
    "    lr=0.001\n",
    "    momentum=0.9\n",
    "    num_channels=3 \n",
    "    num_classes=3\n",
    "    verbose='store_true'\n",
    "    seed=710674\n",
    "\n",
    "args = Args()    \n",
    "\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70151a5a-f25e-4e82-b6d2-d35c01b5c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting torch environment\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device: ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50297e8d-25ee-4164-8a7e-6e3d772ddfcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705ad3f-d762-4fe1-8cba-8c5929893741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f131b38a-fc62-4ce8-a8c3-e61fcc8f315d",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fb6c8-9881-4711-ace9-34c710bc8eaa",
   "metadata": {},
   "source": [
    "### Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4417cf1-5a8f-4103-8191-983c78afcfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eff3 = EfficientNet.from_pretrained('efficientnet-b3', num_classes=args.num_classes)\n",
    "model_resnet18 = models.resnet18(pretrained=True)\n",
    "model_mobnetv2 = models.mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce0c5d-56df-4872-abdb-bfcac48ca6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## resnet 구조는 마지막 fc layer의 out_features 를 바꿔주면 되고.\n",
    "model_resnet18.fc = nn.Linear(in_features = 512, out_features = args.num_classes)\n",
    "model_mobnetv2.classifier = nn.Linear(in_features = 1280, out_features = args.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0c466-ca78-4b73-a00e-31479aa7aba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ae0d61c-e7a4-41fa-8a24-00187646c1d5",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aae3e76-9a73-4b2e-b78b-6d042fff15f4",
   "metadata": {},
   "source": [
    "### Simple CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d489787-2ffc-4b83-8997-eaf6c3b383a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Designing simple CNN model architecture.\n",
    "class CNN_ecg(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(CNN_ecg, self).__init__()\n",
    "\n",
    "        def conv_batch(input_size, output_size, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_size, output_size, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(output_size),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "\n",
    "        def conv_depth(input_size, output_size, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_size, input_size, 3, stride, 1, groups=input_size, bias=False),\n",
    "                nn.BatchNorm2d(input_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                \n",
    "                nn.Conv2d(input_size, output_size, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(output_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            conv_batch(3, 32, 2),\n",
    "            conv_depth(32, 64, 1),\n",
    "            conv_depth(64, 128, 2),\n",
    "            conv_depth(128, 128, 1),\n",
    "            conv_depth(128, 256, 2),\n",
    "            conv_depth(256, 256, 1),\n",
    "            conv_depth(256, 512, 2),\n",
    "            conv_depth(512, 512, 1),\n",
    "            conv_depth(512, 512, 1),\n",
    "            conv_depth(512, 1024, 2),\n",
    "            conv_depth(1024, 1024, 1),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "#         self.fc1 = nn.Linear(1024, 100)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(-1, 1024)\n",
    "#         x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c1a16-1f83-42e2-8d2b-fd63dc47379e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "922f8e24-6919-44b5-acdb-8b1fc2766c5a",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c6ac8-03c3-4243-88a7-1df32dee7f49",
   "metadata": {},
   "source": [
    "### Training Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed26f5c-43a4-4bb3-9357-2f8b5699f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_eff3.to(DEVICE)\n",
    "# model = model_mobnetv2.to(DEVICE)\n",
    "# model = model_resnet18.to(DEVICE)\n",
    "# model = CNN_ecg(args.num_channels, num_classes = args.num_classes).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8cbda-f300-48d8-a53d-b3fbe40563fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f463fa85-d0a8-4cee-ada3-e031e9a81ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "004459b4-8447-4212-bb1d-c2a5b2b5c01d",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70452da4-0be7-409d-8bdb-66157267c5b2",
   "metadata": {},
   "source": [
    "### Input Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe02e6d-7faa-4416-8f70-78d6cfb71456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(256),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(contrast=(0.3, 1), saturation=(0.3, 1)),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456,0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5f590-3361-4b96-9a7a-f0899f25131a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897fb805-20c1-49ed-a895-b47f4c2e25e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75f98b-fe76-43c3-8bce-cb81e9456ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading image data\n",
    "ecg_data = datasets.ImageFolder(root = 'E:/RESEARCH/Datasets/wearable/ECG/test_0420/train', transform = data_transforms)\n",
    "# ecg_data = datasets.ImageFolder(root = 'E:/RESEARCH/Datasets/wearable/ECG/test_0420/train_crop', transform = data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9f889-526a-4b06-95e5-365972a4ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(ecg_data))\n",
    "test_size = len(ecg_data)-train_size\n",
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b753f461-3468-4dbb-ad3b-6af7edc72ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(ecg_data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436a011-6965-4f17-a6fe-e204b9906e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.bs, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.bs, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef1ca4-e3c8-487e-b02d-844947b9845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a7aacc-663c-45a6-b5a1-e07e9bfe88c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353cb358-d690-4705-b373-40ba7e952110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6e28012-a247-47b0-be82-8dc19766a521",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3739b8-a40d-44c3-8f1f-3ae58eb1a31f",
   "metadata": {},
   "source": [
    "### Optimizer and Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c87af-3007-4797-8e73-9adf7d132ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Optimizer and Objective Function\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, total_steps=50, anneal_strategy='cos')\n",
    "criterion = nn.CrossEntropyLoss() ## setup the loss function\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510f129-5344-414b-aa7e-2fa942690238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6700902-ea09-4d61-8e2b-7da68747d37f",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a9f6f-d37f-455c-8dc3-579afcd654a9",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537645c2-462f-4570-b2ba-997e0034e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during CNN model\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))\n",
    "\n",
    "    scheduler.step() #for learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba0939-a6eb-437e-b2cf-fbab13373ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during the learning process\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    validation =[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "            \n",
    "    \n",
    "    test_loss /= (len(test_loader)) \n",
    "    validation_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    validation.append(validation_accuracy)\n",
    "    \n",
    "    return test_loss, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee0e44-78b0-41a4-838a-7b390275e29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904fcda-c97a-4626-9b9b-74e297552492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking train, val loss and accuracy\n",
    "\n",
    "test_los_total = []\n",
    "vali_acc_total = []\n",
    "\n",
    "for epoch in range(1, args.epochs):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, validation_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tValidation Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, validation_accuracy))\n",
    "    \n",
    "    test_los_total.append(test_loss)\n",
    "    vali_acc_total.append(validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa96dc-44bb-4da2-874f-c76d1894da6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee57a2a5-573c-404a-9b40-b50411b264fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACCURACY LISTS\n",
    "# simplecnn_acc = vali_acc_total\n",
    "# resnet18_acc = vali_acc_total\n",
    "# mobnetv2_acc = vali_acc_total\n",
    "effinet3_acc = vali_acc_total\n",
    "\n",
    "### LOSS LISTS\n",
    "# simplecnn_loss = test_los_total\n",
    "# resnet18_loss = test_los_total\n",
    "# mobnetv2_loss = test_los_total\n",
    "effinet3_loss = test_los_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0af00-2ced-4ed2-9ba5-2234ae860afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simplecnn_acc\n",
    "# resnet18_acc\n",
    "# mobnetv2_acc\n",
    "# effinet3_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc289c-cc61-4eef-9afc-ec8f308db2d8",
   "metadata": {},
   "source": [
    "### Accuracy comparison between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37accdf-b3f7-4ce0-abff-7a69c2c81496",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy Graphs\n",
    "max_lim = 70\n",
    "\n",
    "plt.rc('font', family='Times New Roman', serif='Times')\n",
    "plt.rc('xtick', labelsize=16)\n",
    "plt.rc('ytick', labelsize=16)\n",
    "plt.rc('axes', labelsize=20)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.subplots_adjust(left=.15, bottom=-1.16, right=1.99, top=.97)\n",
    "\n",
    "## edit the plot list here\n",
    "plt.plot(range(args.epochs -1), simplecnn_acc)\n",
    "plt.plot(range(args.epochs -1), resnet18_acc)\n",
    "plt.plot(range(args.epochs -1), mobnetv2_acc)\n",
    "plt.plot(range(args.epochs -1), effinet3_acc)\n",
    "\n",
    "\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_xlabel('Number of Epochs')\n",
    "ax.legend(['Simple CNN', 'ResNet-18','MobileNet-v2','EfficientNet-B3'],fontsize=15)\n",
    "sns.set_style('whitegrid')\n",
    "plt.savefig('ecg_entire_acc.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd6ab1-93c4-498d-8a93-1ced79932000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5264ea64-9c09-41ad-88f8-d2e78a0208cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss graphs\n",
    "max_lim = 70\n",
    "\n",
    "plt.rc('font', family='Times New Roman', serif='Times')\n",
    "plt.rc('xtick', labelsize=16)\n",
    "plt.rc('ytick', labelsize=16)\n",
    "plt.rc('axes', labelsize=20)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.subplots_adjust(left=.15, bottom=-1.16, right=1.99, top=.97)\n",
    "\n",
    "## edit the plot list here\n",
    "plt.plot(range(args.epochs -1), simplecnn_loss)\n",
    "plt.plot(range(args.epochs -1), resnet18_loss)\n",
    "plt.plot(range(args.epochs -1), mobnetv2_loss)\n",
    "plt.plot(range(args.epochs -1), effinet3_loss)\n",
    "plt.ylim([0,5])\n",
    "\n",
    "\n",
    "ax.set_ylabel('Test Loss')\n",
    "ax.set_xlabel('Number of Epochs')\n",
    "ax.legend(['Simple CNN', 'ResNet-18','MobileNet-v2','EfficientNet-B3'], fontsize=15)\n",
    "sns.set_style('whitegrid')\n",
    "plt.savefig('ecg_entire_loss.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b05cf1-3c39-40ff-9913-c06221cd62da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b03c0546-e45b-4501-a2e2-e2ea1889c9c2",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b0a9c0-8c05-445b-bd8c-69e9a310575e",
   "metadata": {},
   "source": [
    "### Model Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172c658-70e4-49dc-baf1-57aaf923705e",
   "metadata": {},
   "source": [
    "Performance Measure Explanations\n",
    "\n",
    "* Sensitivity = TP/(TP+FN) = (Number of true positive assessment) / (Number of all posi tive assessment)\n",
    "* Specificity = TN/(TN + FP) = (Number of true negative assessment)/(Number of all negative assessment)\n",
    "* Accuracy = (TN + TP)/(TN+TP+FN+FP) = (Number of correct assessments)/Number of all assessments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b4a56-a74b-4229-9a5f-392e7265fe2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_classes = args.num_classes\n",
    "confusion_matrix = np.zeros((nb_classes, nb_classes))\n",
    "classes = {\n",
    "    \"0\": \"Depression\",\n",
    "    \"1\": \"Normal\",\n",
    "    \"2\": \"Suicidality\"\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (image, label) in enumerate(test_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        outputs = model(image)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for t, p in zip(label.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "print(confusion_matrix)\n",
    "\n",
    "class_names = list(classes.values())\n",
    "df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names).astype(int)\n",
    "heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=8)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=8)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "# plt.savefig('dep_train_entire_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c30c1-92df-4722-a55b-de66a440b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix\n",
    "total = sum(sum(cm))\n",
    "\n",
    "## Accuracy, Sensitivity, and Specificity\n",
    "acc = (cm[0,0]+cm[1,1]+cm[2,2]) / total\n",
    "sen_dep = cm[0,0] / (cm[0,0] + cm[0,1] + cm[0,2])\n",
    "sen_nor = cm[1,1] / (cm[1,0] + cm[1,1] + cm[1,2])\n",
    "sen_sui = cm[2,2] / (cm[2,0] + cm[2,1] + cm[2,2])\n",
    "\n",
    "spe_dep = (cm[1,1] + cm[2,2]) / (cm[1,0] + cm[2,0] + cm[1,1] + cm[2,2])\n",
    "spe_nor = (cm[0,0] + cm[2,2]) / (cm[0,1] + cm[2,1] + cm[0,0] + cm[2,2])\n",
    "spe_sui = (cm[0,0] + cm[1,1]) / (cm[0,2] + cm[1,2] + cm[0,0] + cm[1,1])\n",
    "\n",
    "print(\"Overall classification accuracy is :\", round(acc, 4))\n",
    "print(\"sensitivity of depression class is :\", round(sen_dep, 4))\n",
    "print(\"sensitivity of normal class is :\", round(sen_nor,4))\n",
    "print(\"sensitivity of suicidal class is :\", round(sen_sui,4))\n",
    "\n",
    "print(\"specificity of depression class is :\", round(spe_dep,4))\n",
    "print(\"specificity of normal class is :\", round(spe_nor,4))\n",
    "print(\"specificity of suicidal class is :\", round(spe_sui,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfda23b-db79-46ae-a588-a9cb662616cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d63612af-cfd8-46d2-9481-4123702271ea",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed03d7c-4ec9-49d6-a90d-2e94bdedd7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving pytorch model\n",
    "\n",
    "torch.save(model.state_dict(), 'C:/Users/user/Jupyter/WearableDevice_Analysis/data/d_ecg_simple_entire.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f750f8-ed6b-43bb-8b4d-3eeceff4d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = torch.load('C:/Users/user/Jupyter/WearableDevice_Analysis/data/d_ecg.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca92399-7594-4eec-b769-5d50a7dac074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Optimizer and Objective Function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, total_steps=30,anneal_strategy='cos')\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0630fb-0f04-4ba6-a533-227b0efb0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = 'E:/RESEARCH/Datasets/wearable/ECG/test_0420/val'\n",
    "# data_folder_path = 'E:/RESEARCH/Datasets/wearable/ECG/test_0420/val_crop'\n",
    "test_dataset = datasets.ImageFolder(root=data_folder_path, transform=data_transforms)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=args.bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c254a3-41f5-448e-8dcf-49f379115096",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model testing\n",
    "test_los_total = []\n",
    "test_acc_total = []\n",
    "\n",
    "for epoch in range(1, 30):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model_test, test_dataloader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))\n",
    "    \n",
    "    test_los_total.append(test_loss)\n",
    "    test_acc_total.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb8196b-f6a6-47e6-9d14-3babd9951d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f556a688-61fd-457a-8047-07fd9509c551",
   "metadata": {},
   "source": [
    "### Accuracy and Loss Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ec284-f776-40f5-b5b4-1c0d8d65503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lim = 70\n",
    "\n",
    "plt.rc('font', family='Times New Roman', serif='Times')\n",
    "#plt.rc('text', usetex=True)\n",
    "plt.rc('xtick', labelsize=16)\n",
    "plt.rc('ytick', labelsize=16)\n",
    "plt.rc('axes', labelsize=20)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.subplots_adjust(left=.15, bottom=-1.16, right=1.99, top=.97)\n",
    "\n",
    "plt.plot(range(args.epochs -1), vali_acc_total, '-r')\n",
    "# plt.plot(range(29), test_los_total, '-b')\n",
    "\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_xlabel('Number of Epochs')\n",
    "ax.legend(['Train Accuracy', 'Test Accuracy'],fontsize=15)\n",
    "sns.set_style('whitegrid')\n",
    "# plt.savefig('test_acc.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00c4468-5a00-43ad-8f05-95de740d9787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96fe606-6ec6-4a02-9506-8a21105b1158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcd49093-175f-4b36-8ff6-3c53de1f1d8e",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107cadd-99bc-4c85-aacb-139f651786a4",
   "metadata": {},
   "source": [
    "## Ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da136b-3d2a-45c8-bc88-406bdfd19385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c59a7c-a279-4f1a-ab12-47fb5404db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining ensemble\n",
    "ensemble = VotingClassifier(\n",
    "    estimator = model,\n",
    "    n_estimators = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1aa59-f6ab-4713-a31f-078c91665271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Set the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "ensemble.set_criterion(criterion)\n",
    "ensemble.set_optimizer(\n",
    "    \"Adam\", lr = args.lr\n",
    ")\n",
    "\n",
    "ensemble.set_scheduler(\n",
    "    \"CosineAnnealingLR\", T_max=args.epochs\n",
    ")\n",
    "\n",
    "# Train the ensemble\n",
    "ensemble.fit(\n",
    "    train_loader, epochs=args.epochs\n",
    ")\n",
    "\n",
    "# Evaluate the ensemble\n",
    "acc = ensemble.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c76bc-a50b-49de-b70e-50426d2ed909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b94425-fbc8-43aa-8054-1761b4c35d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_list=[]\n",
    "    for i, (image, label) in enumerate(test_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        outputs = model(image)\n",
    "        \n",
    "        batch_idx = args.bs * i      #batch size만큼 채워놓고\n",
    "        prediction = (outputs > 0.5) #\n",
    "        prediction_array[batch_idx: batch_idx + image.shape[0], :] = prediction.astype(int)\n",
    "        predictions_list.append(prediction_array[..., np.newaxis])\n",
    "        \n",
    "        \n",
    "#         _, preds = torch.max(outputs, 1)\n",
    "#         for t, p in zip(label.view(-1), preds.view(-1)):\n",
    "#                 confusion_matrix[t.long(), p.long()] += 1\n",
    "                \n",
    "#  axis = 2를 기준으로 평균\n",
    "predictions_array = np.concatenate(predictions_list, axis = 2)\n",
    "predictions_mean = predictions_array.mean(axis = 2)\n",
    "\n",
    "# 평균 값이 0.5보다 클 경우 1 작으면 0\n",
    "predictions_mean = (predictions_mean > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff891a-2efe-427e-b7bc-c5823bf74714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053666d7-2496-4dd2-aa6e-6b9152826a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc83e5a-5034-4aa0-b9eb-1587be9f8350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c5c39-5ce8-4b17-a1d3-e0dcd4f2b8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d5ad07-4f53-445e-89d1-511410ddbfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2844fe8-1f20-41e4-9021-983d25038c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e692c-f3e8-4053-b9bf-2635bd855fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4cfae-1ccd-40cf-a16c-6f247113c76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0921df3d-af02-445d-bd45-98f56ee598c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0862ba4f-8787-4241-b4a5-bb9f7fdd72d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca54703c-82f9-4925-b887-b4b693ab38d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3dc8b-61ce-425c-b0ee-5b8695f1491d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6be033-c1d6-4602-b88e-84accab1268c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb5ea26-b16d-4bba-877f-2f7a1023def8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e488d3-2084-4990-94f3-9175bb3b9804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d687c0ba-b09e-414c-bc65-a79e7c38b187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92456f7-2340-4737-bf9e-cd12668cf593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
