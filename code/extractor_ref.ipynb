{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b43ed6-0865-4b16-8a94-31486cf450ba",
   "metadata": {},
   "source": [
    "## DATA EXTRACTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a168c-ad89-43a1-b501-43f1eb2ad432",
   "metadata": {},
   "source": [
    "ref: https://github.com/BlackFireAlex/samsung_health_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286bd9af-f87d-4ee5-a92e-7fed7bab619b",
   "metadata": {},
   "source": [
    "1. Download data from s.h application\n",
    "2. Place the folder at the root\n",
    "3. Run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbda737a-d329-40da-bd97-94e5dc8401fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import os.path\n",
    "import sqlite3\n",
    "from functools import reduce\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aaadd0-7882-4517-a963-188263200106",
   "metadata": {},
   "outputs": [],
   "source": [
    "global samsung_csv_paths\n",
    "# samsung_data_path = './Samsung Health'\n",
    "samsung_data_path = 'E:/RESEARCH/Datasets/wearable/Samsung Health'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebec5c7-230d-4dae-9f85-5e3b271f20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_codes = {\n",
    "    14001: 'Swimming',\n",
    "    1001: 'Walking',\n",
    "    0: 'Other workout',\n",
    "    11007: 'Cycling',\n",
    "    1002: 'Running'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde52d36-c081-4ae7-9037-507531328fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_date(x, col_name=''):\n",
    "    try:\n",
    "        if any('day_time' in col for col in x.columns):\n",
    "            col_name = 'day_time'\n",
    "            x['day_time'] = pd.to_datetime(\n",
    "                x[[col for col in x.columns if col_name in col][0]], unit='ms')\n",
    "        if any('start_time' in col for col in x.columns):\n",
    "            col_name = 'start_time'\n",
    "            x['day_time'] = pd.to_datetime(\n",
    "                x[[col for col in x.columns if col_name in col][0]])\n",
    "        x['day_time'] = pd.to_datetime(x['day_time'])\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        print(\"Can't find one of these columns\")\n",
    "        print(\"Column names\", x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e429f65-014a-41b5-8ae3-c5ee8056d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for daily sport data\n",
    "def get_json(s):\n",
    "    join = os.path.join('**', s + '*.json')\n",
    "    samsung_json_paths = glob(join, recursive=True)\n",
    "    json_file = open(samsung_json_paths[0])\n",
    "    return json.load(json_file)\n",
    "\n",
    "\n",
    "def get_daily_sports_data(data, day):\n",
    "    extra_data = None\n",
    "    sport_counts = dict()\n",
    "    sports_time = dict()\n",
    "\n",
    "    try:\n",
    "\n",
    "        s = str(data[(data['day_time'] == pd.to_datetime(day)) & (data['datauuid_y'] != np.nan)]['datauuid_y'].iloc[0])\n",
    "        extra_data = get_json(s)\n",
    "\n",
    "        for d in extra_data['mActivityList']:\n",
    "            activity_time = d['mActiveTime'] / 1000 / 60\n",
    "            sport_counts['day_time'] = day\n",
    "            sports_time['day_time'] = day\n",
    "            try:\n",
    "                sport_counts[sports_codes[d['mType']] + ' count'] += 1\n",
    "            except KeyError:\n",
    "                sport_counts[sports_codes[d['mType']] + ' count'] = 1\n",
    "            try:\n",
    "                sports_time[sports_codes[d['mType']]] += activity_time\n",
    "            except KeyError:\n",
    "                sports_time[sports_codes[d['mType']]] = activity_time\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    return sports_time, sport_counts\n",
    "\n",
    "\n",
    "def get_all_daily_sport_data():\n",
    "    from datetime import date, timedelta\n",
    "    all_health_data_concat = get_health_data()\n",
    "\n",
    "    all_sports_data_dict = dict()\n",
    "    for sport in sports_codes.values():\n",
    "        all_sports_data_dict[sport] = []\n",
    "    all_sports_data_dict['day_time'] = []\n",
    "    start_date = pd.to_datetime(\"2021-01-01\")\n",
    "    end_date = all_health_data_concat['day_time'].dt.date.max()\n",
    "    delta = timedelta(days=1)\n",
    "    while start_date <= end_date:\n",
    "        current_date_s = start_date.strftime(\"%Y-%m-%d\")\n",
    "        daily_sport_dict = get_daily_sports_data(all_health_data_concat, current_date_s)[0]\n",
    "        all_sports_data_dict['day_time'].append(start_date)\n",
    "        for k in sports_codes.values():\n",
    "            if k in daily_sport_dict.keys():\n",
    "                all_sports_data_dict[k].append(daily_sport_dict[k])\n",
    "            else:\n",
    "                all_sports_data_dict[k].append(0)\n",
    "\n",
    "        start_date += delta\n",
    "\n",
    "    all_df_merged = pd.DataFrame().from_dict(all_sports_data_dict)\n",
    "    return all_df_merged\n",
    "\n",
    "\n",
    "def get_weight_data():\n",
    "    weight_data_path = os.path.join(samsung_data_path, '**', '*weight*.csv')\n",
    "    paths = glob(weight_data_path, recursive=True)\n",
    "    paths.sort(key=len)\n",
    "    w_data = sam_read_csv(paths[0])\n",
    "    conv_date(w_data)\n",
    "    return w_data\n",
    "\n",
    "\n",
    "def get_samsung_df_merged():\n",
    "    global samsung_csv_paths\n",
    "\n",
    "    first_csv = sam_read_csv(samsung_csv_paths[10])\n",
    "    all_csv_df = {os.path.basename(j).replace('com.samsung.', ''): sam_read_csv(j) for j in samsung_csv_paths}\n",
    "\n",
    "    key_words_csv = [\n",
    "        'com.samsung.health.weight'\n",
    "        , 'com.samsung.shealth.activity.day_summary'\n",
    "        , 'com.samsung.shealth.calories_burned.details'\n",
    "    ]\n",
    "\n",
    "    all_df_of_interest = [v for k, v in all_csv_df.items() if\n",
    "                          any(x.replace('com.samsung.', '') in k for x in key_words_csv)]\n",
    "\n",
    "    for i, df in enumerate(all_df_of_interest):\n",
    "        conv_date(df)\n",
    "\n",
    "    all_df_merged = reduce(lambda left, right: pd.merge(left, right, on=['day_time'],\n",
    "                                                        how='outer'), all_df_of_interest)\n",
    "    return all_df_merged\n",
    "\n",
    "\n",
    "def get_health_data():\n",
    "    h_data = get_samsung_df_merged()\n",
    "    h_data = h_data[[col for col in h_data.columns if \"mass\" not in col]]\n",
    "    col_values = h_data.columns\n",
    "    h_data.columns = [s.replace('com.samsung.shealth.', '') for s in col_values]\n",
    "    col = [col for col in h_data.columns if len(h_data[col].unique()) >= 2]\n",
    "    h_data = h_data[col]\n",
    "    h_data['day_time'] = pd.to_datetime(h_data['day_time'])\n",
    "    h_data = pd.merge(h_data, get_heart_rate_data(), how=\"outer\", on=\"day_time\")\n",
    "    return h_data\n",
    "\n",
    "\n",
    "def extract_activity(record):\n",
    "    try:\n",
    "        return record[\"activity\"][0][\"activity\"][0][\"type\"]\n",
    "    except:\n",
    "        return \"MISSING\"\n",
    "\n",
    "\n",
    "def get_heart_rate_data():\n",
    "    hr_data_path = os.path.join(samsung_data_path, '**', 'com.samsung.shealth.tracker.heart_rate', '**', '*.json')\n",
    "    hr_file_paths = glob(hr_data_path, recursive=True)\n",
    "    hr_df = pd.DataFrame(columns=['heart_rate', 'heart_rate_max', 'heart_rate_min', 'start_time', 'end_time'])\n",
    "    hr_df = hr_df.fillna(0)\n",
    "    for f in hr_file_paths:\n",
    "        try:\n",
    "            with open(f) as file:\n",
    "                json_hr = json.load(file)\n",
    "                part_df = pd.DataFrame(json_hr)\n",
    "                hr_df = pd.concat([hr_df, part_df])\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "    hr_df['day_time'] = pd.to_datetime(hr_df['start_time'], unit='ms')\n",
    "    return hr_df\n",
    "\n",
    "\n",
    "def get_heart_rate_data_by_hour():\n",
    "    hr_df = get_heart_rate_data()\n",
    "    times = pd.DatetimeIndex(hr_df.day_time)\n",
    "    hr_df = hr_df.groupby([times.hour, times.date]).agg('mean')\n",
    "    hr_df.reset_index(inplace=True)\n",
    "    hr_df['day_time'] = pd.to_datetime(\n",
    "        pd.to_datetime(hr_df['level_1']).dt.strftime('%Y-%m-%d') + ' ' + hr_df['day_time'].astype(str) + ':00')\n",
    "\n",
    "    return hr_df\n",
    "\n",
    "\n",
    "# Non aggregated data\n",
    "def get_heart_rate_data_raw():\n",
    "    hr_df = get_heart_rate_data()\n",
    "    return hr_df\n",
    "\n",
    "\n",
    "def get_exercise_data():\n",
    "    exercise_paths = os.path.join(samsung_data_path, '**', '*exercise*.csv')\n",
    "    all_paths = glob(exercise_paths, recursive=True)\n",
    "    all_paths.sort(key=len)\n",
    "    cal = sam_read_csv(all_paths[0])\n",
    "    cal = cal[[\"com.samsung.health.exercise.time_offset\", \"com.samsung.health.exercise.start_time\",\n",
    "               \"com.samsung.health.exercise.end_time\",\n",
    "               \"com.samsung.health.exercise.calorie\"]]\n",
    "\n",
    "    a = cal['com.samsung.health.exercise.time_offset'].astype(str).str[5].astype(int)\n",
    "    cal[\"com.samsung.health.exercise.end_time\"] = pd.to_datetime(\n",
    "        cal[\"com.samsung.health.exercise.end_time\"]) + pd.TimedeltaIndex(\n",
    "        cal['com.samsung.health.exercise.time_offset'].astype(str).str[5].astype(int) - 1, unit='H')\n",
    "\n",
    "    cal[\"com.samsung.health.exercise.start_time\"] = pd.to_datetime(\n",
    "        cal[\"com.samsung.health.exercise.start_time\"]) + pd.TimedeltaIndex(\n",
    "        cal['com.samsung.health.exercise.time_offset'].astype(str).str[5].astype(int) - 1, unit='H')\n",
    "    return cal\n",
    "\n",
    "\n",
    "def get_food_intake_data():\n",
    "    cal_data_path = os.path.join(samsung_data_path, '**', '*food_intake*.csv')\n",
    "    f_path = glob(cal_data_path, recursive=True)[0]\n",
    "    cal_data = sam_read_csv(f_path)\n",
    "    conv_date(cal_data)\n",
    "    cal_data = cal_data.groupby(cal_data['day_time'].dt.date).agg('sum')\n",
    "    cal_data.reset_index(inplace=True)\n",
    "    cal_data = cal_data[cal_data['calorie'] > 0]\n",
    "    cal_data['day_time'] = pd.to_datetime(cal_data['day_time'])\n",
    "    return cal_data\n",
    "\n",
    "\n",
    "def get_burned_calories_data():\n",
    "    burned_data_path = os.path.join(samsung_data_path, '**', '*burned.details*.csv')\n",
    "    f_path = glob(burned_data_path, recursive=True)[0]\n",
    "    burned_data = sam_read_csv(f_path)\n",
    "    conv_date(burned_data)\n",
    "    burned_data['total_burned'] = burned_data['com.samsung.shealth.calories_burned.active_calorie'] + burned_data[\n",
    "        'com.samsung.shealth.calories_burned.rest_calorie'] + burned_data[\n",
    "                                      'com.samsung.shealth.calories_burned.tef_calorie']\n",
    "    return burned_data\n",
    "\n",
    "\n",
    "def get_caloric_balance_daily_data():\n",
    "    combined_data = pd.merge(get_burned_calories_data(), get_food_intake_data(), on='day_time')\n",
    "    combined_data['balance'] = combined_data['calorie'] - combined_data['total_burned']\n",
    "    combined_data = combined_data[\n",
    "        ['balance', 'total_burned', 'com.samsung.shealth.calories_burned.active_calorie', 'calorie', 'day_time']]\n",
    "    print(\"You probably lost about \", combined_data['balance'].sum() / 7250, \" during this period !\")\n",
    "    print(combined_data['day_time'].min(), \" to \", combined_data['day_time'].max())\n",
    "    return combined_data\n",
    "\n",
    "\n",
    "def get_sleep_data():\n",
    "    global samsung_csv_paths\n",
    "    sleep_csv = [path for path in samsung_csv_paths if 'sleep.' in path][1]\n",
    "    sleep_df = sam_read_csv(sleep_csv)\n",
    "\n",
    "    sleep_df.columns = [col.split('.')[-1] for col in sleep_df.columns]\n",
    "\n",
    "    time_stamp_cols = ['create_time', 'end_time', 'start_time', 'update_time']\n",
    "\n",
    "    for col_name in time_stamp_cols:\n",
    "        sleep_df[col_name] = pd.to_datetime(sleep_df[col_name])\n",
    "\n",
    "    sleep_df['sleep_duration'] = sleep_df['sleep_duration'].fillna(0)\n",
    "    sleep_df['sleep_duration'] = sleep_df['sleep_duration'] / 60\n",
    "    sleep_df.dropna(axis=1, inplace=True, )\n",
    "\n",
    "    sleep_df['weekday'] = [day.weekday() for day in sleep_df['start_time']]\n",
    "\n",
    "    sleep_df.sort_values(by=['start_time'], inplace=True)\n",
    "    sleep_df.reset_index(inplace=True)\n",
    "\n",
    "    sleep_df['sleep_gap'] = (sleep_df['start_time'].shift(-1) - sleep_df['end_time']) / np.timedelta64(1, 'h')\n",
    "\n",
    "    sleep_df['Disrupted'] = (sleep_df.sleep_gap < 2) & (sleep_df.sleep_duration < 4)\n",
    "\n",
    "    sleep_df['weekday'] = sleep_df['weekday'].map({0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\",\n",
    "                                                   3: \"Thursday\", 4: \"Friday\", 5: \"Saturday\", 6: \"Sunday\"})\n",
    "    times = pd.DatetimeIndex(sleep_df.start_time)\n",
    "    sleep_df = sleep_df.groupby(times.date).agg('sum')\n",
    "    sleep_df.reset_index(inplace=True)\n",
    "\n",
    "    sleep_df['day_time'] = sleep_df['level_0']\n",
    "    sleep_df.drop(['level_0', 'index'], axis=1, inplace=True)\n",
    "    return sleep_df\n",
    "\n",
    "\n",
    "def sam_read_csv(x):\n",
    "    return pd.read_csv(x, skiprows=1, sep=\",\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3405ed83-4f3b-4ce6-aec6-16a9ff3cc928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data():\n",
    "    global samsung_csv_paths\n",
    "    samsung_base_dir = os.path.join(samsung_data_path)\n",
    "\n",
    "    samsung_dump_dirs = glob(os.path.join(samsung_base_dir, '*'))\n",
    "    samsung_dump_dir = os.path.basename(samsung_dump_dirs[0])\n",
    "    print(len(samsung_dump_dirs), 'dumps found, taking first:', samsung_dump_dir)\n",
    "\n",
    "    samsung_csv_paths = glob(os.path.join(samsung_base_dir, samsung_dump_dir, '*.csv'))\n",
    "    print(len(samsung_csv_paths), 'csvs found')\n",
    "    blood_sugar_data = pd.DataFrame(columns=['empty', 'empty', 'empty'])\n",
    "\n",
    "    hr_data = get_heart_rate_data_raw().sort_values(by='day_time', ascending=True)\n",
    "    sleep_data = get_sleep_data()\n",
    "    daily_sports_data = get_all_daily_sport_data()\n",
    "    food_intake_data = get_food_intake_data()\n",
    "    exercise_data = get_exercise_data()\n",
    "    caloric_balance_data = get_caloric_balance_daily_data().sort_values(by='day_time', ascending=True)\n",
    "    weight_data = get_weight_data()\n",
    "    weight_data = weight_data[weight_data['weight'] > 0][['weight', 'day_time']] \\\n",
    "        .sort_values(by='day_time', ascending=False)\n",
    "\n",
    "    df_data_list = [blood_sugar_data, hr_data, sleep_data, daily_sports_data,\n",
    "                    food_intake_data, exercise_data, caloric_balance_data, weight_data]\n",
    "    df_name_list = ['blood_sugar_data', 'hr_data', 'sleep_data',\n",
    "                    'daily_sports_data', 'food_intake_data', 'exercise_data', 'caloric_balance_data', 'weight_data']\n",
    "\n",
    "    for i, d in enumerate(df_data_list):\n",
    "        try:\n",
    "            d.to_csv('./' + str(df_name_list[i]) + '.csv')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e301e-3b68-4fd0-80a9-f330eb6578f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de787e2d-4468-4a30-9486-345e9e141955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a7aedb-c569-4e98-84d4-42f81810fea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49c0af-2148-42d0-a3ed-a178f47b5df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5877225-7b55-4cd0-a868-36a672cf1723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6e264-96aa-455b-9eda-cdbb97e3baae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac12367-e966-4210-abe1-71b8fae5a48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f63d3-5b89-4acf-8601-312721ac4997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a67d3-ab42-423a-83ec-589acc03c4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0179a09-a33a-4bc8-b724-93799e49c9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85a0e31-405b-469e-bc24-732b68ecf3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db361371-2f3c-443b-a621-b075143c610b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ebe9d8-da0b-445a-a610-712d4b19e071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f53d3-3abc-4229-b736-3408ff439068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab619b-d962-47d2-8033-6b1910f56272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1152f-4727-4d9e-a77a-d9aebb8d3dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4f559-91f0-4f10-881e-3ff695cc35df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de43a020-61a9-4588-ae63-2db8bfe02937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc07e2-ba58-4168-b14d-a20d10afc06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8adb0f2-45bd-4b86-92a9-b860a8a09ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b2abc7-f926-4e68-85c3-37169ce0e670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb45fd-e2ec-44d5-bb6c-06958ce8ab0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3471b1b-0d9d-438c-b941-5feadd8909be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
